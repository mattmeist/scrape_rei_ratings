{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTES ###\n",
    "# This code is far from perfect. It works well enough for the purpose it was designed for.\n",
    "# However, I am far from the best programmer, and am far more comfortable in R than Python. \n",
    "# That is all just to say that this is not text book, but I hope it is good enough to help someone scrape some data.\n",
    "# If you do see errors here or in the other scripts, please reach out to me at matt.meister@colorado.edu\n",
    "\n",
    "## The best way to use this to learn: ##\n",
    "# Would be to take code out of the for loops it is in, and run things piece by piece\n",
    "# Do so with the REI website up alongside, preferably while able to see the source code (i.e., right click \"inspect\")\n",
    "\n",
    "## Citing ##\n",
    "# If you use this code or data in a way that feels necessary to cite, please do so as:\n",
    "# Meister, M., & Reinholtz, N. (2022). Quality in Context: Evidence that Consumption Context Influences User-Generated Product Ratings. Available at SSRN 4155522.\n",
    "# The most updated version of that paper can be found here: https://drive.google.com/file/d/1BfDzIxTsCtQOMRwkbAlK12xDKWDXSbSN/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing necessary stuff\n",
    "import json # A lot of data online is stored as jsons\n",
    "import pandas as pd\n",
    "import re # Regular expressions, to help us parse html, json.\n",
    "import requests # To pull internet pages\n",
    "import time # To allow us to \"sleep\" the scraping, so as not to overwhelm the servers\n",
    "import os # To create folders\n",
    "import math\n",
    "from bs4 import BeautifulSoup # To make html easier to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'PRICES' already exists.\n",
      "Directory 'JSONs' already exists.\n",
      "Directory 'QUESTIONS' already exists.\n",
      "Directory 'REVIEWS' already exists.\n",
      "Directory 'OVRs' already exists.\n"
     ]
    }
   ],
   "source": [
    "products = [] # We'll put products in here\n",
    "larger_categories = [] # This is where we'll put overarching categories. \n",
    "# I don't want them, since they'll lead to a lot of duplicate scraping\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15'}\n",
    "\n",
    "# Scrape categories\n",
    "html = BeautifulSoup(requests.get('https://www.rei.com/categories', headers = headers).text, 'html.parser')\n",
    "\n",
    "# Find overarchign categories\n",
    "larger_list = html.find_all('div', class_=\"all-categories__parent-header\")\n",
    "larger_categories = [\"https://www.rei.com\" + tag.find('a').get('href','') if tag.find('a') else '' for tag in larger_list]\n",
    "\n",
    "\n",
    "# Find all categories\n",
    "product_list = html.find_all('a', class_='cdr-link_13-5-3 cdr-link--standalone_13-5-3')\n",
    "products = [\"https://www.rei.com\" + d['href'] for d in product_list]\n",
    "\n",
    "# Remove overarching categories\n",
    "products = [x for x in products if x not in larger_categories]\n",
    "products = [re.compile(r'https://www\\.rei\\.com/c/').sub('', url) for url in products]\n",
    "\n",
    "# Set up the rest of the scraping!\n",
    "# Set the base url to call reviews from their hosting site\n",
    "url = 'https://api.bazaarvoice.com/data/batch.json'\n",
    "# Give yourself something to not look like a bot\n",
    "    \n",
    "# Function to create a directory if it doesn't exist\n",
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "        print(f\"Directory '{directory}' created.\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "create_directory('PRICES')\n",
    "create_directory('JSONs')\n",
    "create_directory('QUESTIONS')\n",
    "create_directory('REVIEWS')\n",
    "create_directory('OVRs')\n",
    "\n",
    "for product in products:\n",
    "    page = 1\n",
    "    product_hrefs = []\n",
    "    pids = []\n",
    "    full_prices = []\n",
    "    sale_prices = []\n",
    "    compare_prices = []\n",
    "    \n",
    "    # Get links for all results pages\n",
    "    html = BeautifulSoup(requests.get(f'https://www.rei.com/c/{product}?pagesize=90',\n",
    "                                     headers = headers).text, 'html.parser')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Try to see how many pages we have\n",
    "    try:\n",
    "        pagination = html.find('div', {'data-id': 'pagination-test-count'}).get_text(strip=True)\n",
    "        num_pg = math.ceil(int(re.findall(r'\\b\\d+\\b', pagination)[2])/int(re.findall(r'\\b\\d+\\b', pagination)[1]))\n",
    "        pages = list(range(1,int(num_pg)+1))\n",
    "    except:\n",
    "        pages = list(range(1,int(1)+1))\n",
    "    \n",
    "    # Go through each page, scrape all product links\n",
    "    for page in pages:\n",
    "        html = BeautifulSoup(requests.get(f'https://www.rei.com/c/{product}?pagesize=90&page={page}',\n",
    "                                         headers = headers).text, 'html.parser')\n",
    "        html_contents = html.find_all('li', class_ = \"VcGDfKKy_dvNbxUqm29K\")\n",
    "\n",
    "        for html_content in html_contents:\n",
    "            html_product = str(html_content)\n",
    "\n",
    "            href_match = re.compile(r'href=\"(/product/\\d+/[a-zA-Z0-9-]+)\"').search(html_product)\n",
    "            if href_match:\n",
    "                product_hrefs.append(href_match.group(1))\n",
    "                pids.append(re.search(r\"(\\d{6})\", href_match.group(1))[0])\n",
    "            else:\n",
    "                product_hrefs.append('NA')\n",
    "                pids.append('NA')\n",
    "\n",
    "            sale_price = re.compile(r'data-ui=\"sale-price\">(\\$[\\d.]+)').search(html_product)\n",
    "            if sale_price:\n",
    "                sale_prices.append(sale_price.group(1))\n",
    "            else:\n",
    "                sale_prices.append('NA')\n",
    "\n",
    "            compare_price = re.compile(r'data-ui=\"compare-at-price\">(\\$[\\d.]+)').search(html_product)\n",
    "            if compare_price:\n",
    "                compare_prices.append(compare_price.group(1))\n",
    "            else:\n",
    "                compare_prices.append('NA')\n",
    "\n",
    "            full_price = re.compile(r'data-ui=\"full-price\">(\\$[\\d.]+)').search(html_product)\n",
    "            if full_price:\n",
    "                full_prices.append(full_price.group(1))\n",
    "            else:\n",
    "                full_prices.append('NA')\n",
    "\n",
    "        # Create the data frame of prices for each category, and write it. \n",
    "    pricesDF = pd.DataFrame({'pid':pids,\n",
    "                             'href':product_hrefs,\n",
    "                            'full_price':full_prices,\n",
    "                            'sale_price':sale_prices,\n",
    "                            'compare_price':compare_prices})\n",
    "\n",
    "    pricesDF.to_csv(f'PRICES/REI_{product}_prices.csv')\n",
    "\n",
    "    \n",
    "    # Now the good stuff! Call the API to get reviews, questions, other stuff\n",
    "    for pid in pids:\n",
    "        params = {\n",
    "            # TO GET PARAMETER INFO:\n",
    "            # 1) GO TO A PRODUCT ON REI.COM\n",
    "            # 2) RIGHT CLICK\n",
    "            # 3) INSPECT ELEMENT (ON SAFARI)\n",
    "            # 4) GO TO NETWORK > BATCH.JSON > HEADERS\n",
    "        'passkey': 'thvpbov9ywkkl4nkhbeq0wm1i',#This changes semi-frequently\n",
    "        'apiversion': '5.5',\n",
    "        'displaycode': '15372-en_us',\n",
    "        'resource.q0': 'products',\n",
    "        'filter.q0': f'id:eq:{pid}',\n",
    "        'stats.q0': 'questions,reviews',\n",
    "        'filteredstats.q0': 'questions,reviews',\n",
    "        'filter_questions.q0': 'contentlocale:eq:en*,en_US',\n",
    "        'filter_answers.q0': 'contentlocale:eq:en*,en_US',\n",
    "        'filter_reviews.q0': 'contentlocale:eq:en*,en_US',\n",
    "        'filter_reviewcomments.q0': 'contentlocale:eq:en*,en_US',\n",
    "        'resource.q1': 'questions',\n",
    "        'filter.q1': f'productid:eq:{pid}',\n",
    "        'sort.q1': 'totalanswercount:desc',\n",
    "        'stats.q1': 'questions',\n",
    "        'filteredstats.q1': 'questions',\n",
    "        'include.q1': 'authors,products,answers',\n",
    "        'filter_questions.q1': 'contentlocale:eq:en*,en_US',\n",
    "        'filter_answers.q1': 'contentlocale:eq:en*,en_US',\n",
    "        'limit.q1': '20',\n",
    "        'offset.q1': '0',\n",
    "        'limit_answers.q1': '10',\n",
    "        'resource.q2': 'reviews',\n",
    "        'filter.q2': f'productid:eq:{pid}',\n",
    "        'sort.q2': 'submissiontime:desc',\n",
    "        'stats.q2': 'reviews',\n",
    "        'filteredstats.q2': 'reviews',\n",
    "        'include.q2': 'authors,products,comments',\n",
    "        'filter_reviews.q2': 'contentlocale:eq:en*,en_US',\n",
    "        'filter_reviewcomments.q2': 'contentlocale:eq:en*,en_US',\n",
    "        'filter_comments.q2': 'contentlocale:eq:en*,en_US',\n",
    "        'limit.q2': '100',\n",
    "        'offset.q2': '0',\n",
    "        'limit_comments.q2': '20',\n",
    "        'callback': 'JSONPHandler'\n",
    "    }\n",
    "        #Get that stuff!\n",
    "        req = requests.get(url, headers = headers, params = params)\n",
    "        data_json = req.text.split(\"(\", 1)[1].strip(\")\") # Convert to json\n",
    "        batched_results = json.loads(data_json)['BatchedResults']\n",
    "        \n",
    "        # Write the raw JSON so you can use it later if needed\n",
    "        if batched_results['q0']['Results']==[]:\n",
    "            continue\n",
    "        products_json = batched_results['q0']['Results'][0]\n",
    "        with open(f'JSONs/REI_{pid}_batched_results.json', 'w') as outfile:\n",
    "            json.dump(products_json, outfile)\n",
    "        \n",
    "        # Scrape questions to use if desired\n",
    "        questions = pd.DataFrame(batched_results['q1']['Results'])\n",
    "        questions.to_csv(f'QUESTIONS/REI_{pid}_questions.csv')  # Write them as csv\n",
    "        \n",
    "        # Scrape reviews\n",
    "        reviews = pd.DataFrame(batched_results['q2']['Results'])\n",
    "        reviews.to_csv(f'REVIEWS/REI_{pid}_reviews.csv')  \n",
    "        \n",
    "        # Product overall information\n",
    "        ovr = pd.DataFrame({'Name': 'NA' if products_json['Name'] == {} else[products_json['Name']],\n",
    "                            'Active': 'NA' if products_json['Active'] == {} else[products_json['Active']],\n",
    "                            'Brand': 'NA' if products_json['Brand'] == {} else [products_json['Brand']['Id']],\n",
    "                            'reviews': 'NA' if products_json['TotalReviewCount'] == {} else [products_json['TotalReviewCount']],\n",
    "                            'pid': f'{pid}', \n",
    "                            'ovr': 'NA' if products_json['ReviewStatistics']['AverageOverallRating'] == {} else[products_json['ReviewStatistics']['AverageOverallRating']],\n",
    "                           'desc': 'NA' if products_json['Description'] == {} else [products_json['Description']],\n",
    "                           'ProductPageUrl': 'NA' if products_json['ProductPageUrl'] == {} else [products_json['ProductPageUrl']],\n",
    "                            'category': f'{product}',\n",
    "                           })\n",
    "        ovr.to_csv(f'OVRs/REI_{pid}_ovr.csv')\n",
    "        time.sleep(8) #Rest, so that you don't overload REI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
