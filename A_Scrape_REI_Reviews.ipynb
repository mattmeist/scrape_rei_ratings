{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTES ###\n",
    "# This code is far from perfect. It works well enough for the purpose it was designed for.\n",
    "# However, I am far from the best programmer, and am far more comfortable in R than Python. \n",
    "# That is all just to say that this is not text book, but I hope it is good enough to help someone scrape some data.\n",
    "# If you do see errors here or in the other scripts, please reach out to me at matt.meister@colorado.edu\n",
    "\n",
    "## The best way to use this to learn: ##\n",
    "# Would be to take code out of the for loops it is in, and run things piece by piece\n",
    "# Do so with the REI website up alongside, preferably while able to see the source code (i.e., right click \"inspect\")\n",
    "\n",
    "## Citing ##\n",
    "# If you use this code or data in a way that feels necessary to cite, please do so as:\n",
    "# Meister, M., & Reinholtz, N. (2022). Quality in Context: Evidence that Consumption Context Influences User-Generated Product Ratings. Available at SSRN 4155522.\n",
    "# The most updated version of that paper can be found here: https://drive.google.com/file/d/1BfDzIxTsCtQOMRwkbAlK12xDKWDXSbSN/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing necessary stuff\n",
    "import json # A lot of data online is stored as jsons\n",
    "import pandas as pd\n",
    "import re # Regular expressions, to help us parse html, json.\n",
    "import requests # To pull internet pages\n",
    "import time # To allow us to \"sleep\" the scraping, so as not to overwhelm the servers\n",
    "import os # To create folders\n",
    "from bs4 import BeautifulSoup # To make html easier to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "products = [] # We'll put products in here\n",
    "larger_categories = [] # This is where we'll put overarching categories. \n",
    "# I don't want them, since they'll lead to a lot of duplicate scraping\n",
    "\n",
    "# Scrape categories\n",
    "html = BeautifulSoup(requests.get('https://www.rei.com/categories').text, 'html.parser')\n",
    "\n",
    "# Find overarchign categories\n",
    "larger_list = html.find_all('div', class_=\"all-categories__parent-header\")\n",
    "larger_categories = [\"https://www.rei.com\" + tag.find('a').get('href','') if tag.find('a') else '' for tag in larger_list]\n",
    "\n",
    "\n",
    "# Find all categories\n",
    "product_list = html.find_all('a', class_='cdr-link_13-0-3 cdr-link--standalone_13-0-3')\n",
    "products = [\"https://www.rei.com\" + d['href'] for d in product_list]\n",
    "\n",
    "# Remove overarching categories\n",
    "products = [x for x in products if x not in larger_categories]\n",
    "\n",
    "# Set up the rest of the scraping!\n",
    "# Set the base url to call reviews from their hosting site\n",
    "url = 'https://api.bazaarvoice.com/data/batch.json'\n",
    "# Give yourself something to not look like a bot\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15'}\n",
    "    \n",
    "page = 1 # This is for navigation later\n",
    "bags = [] # This will become a list of links for all products in a category\n",
    "pids = [] # This will become a list of links for all product IDs in a category\n",
    "prices = [] # This will become a list of links for all prices in a category\n",
    "\n",
    "os.mkdir(f'PRICES') #To store price data \n",
    "#NOTE: This ^ is a bad way to store prices, but it's fine for my purpose. \n",
    "# You might prefer storing prices in the OVRs files.\n",
    "os.mkdir(f'JSONs') #To store JSON data for all products\n",
    "os.mkdir(f'QUESTIONS') #To store questions data\n",
    "os.mkdir(f'REVIEWS') #To store reviews data\n",
    "os.mkdir(f'OVRs') #To store overall product info data (summarizing the JSONs)\n",
    "\n",
    "for product in products:\n",
    "    os.mkdir(f'REI_{product}') # Make folders for each product category. Note that this is personal preference\n",
    "    os.mkdir(f'REI_{product}/REI_{product}_products')\n",
    "    os.mkdir(f'REI_{product}/REI_{product}_questions')\n",
    "    os.mkdir(f'REI_{product}/REI_{product}_reviews')\n",
    "    os.mkdir(f'REI_{product}/REI_{product}_ovr')\n",
    "    page = 1\n",
    "    bags = []\n",
    "    pids = []\n",
    "    prices = []\n",
    "    \n",
    "    # Get links for all results pages\n",
    "    html = BeautifulSoup(requests.get(f'https://www.rei.com/c/{product}?pagesize=90').text, 'html.parser')\n",
    "    time.sleep(1)\n",
    "    num_pg = 1 if html.find('a', class_='e98NTAHiDGsqQqLsoyvz-') == None else html.find('a', class_='e98NTAHiDGsqQqLsoyvz-')['href'][-1]\n",
    "    pages = list(range(1,int(num_pg)+1))\n",
    "    \n",
    "    # Go through each page, scrape all product links\n",
    "    for page in pages:\n",
    "        html = BeautifulSoup(requests.get(f'https://www.rei.com/c/{product}?pagesize=90&page={page}').text, 'html.parser')\n",
    "        bags = bags + ['https://www.rei.com' + d['href'] for d in html.find_all('a', class_=\"_1A-arB0CEJjk5iTZIRpjPs _1K5N3WSl_8ywawYr0tzSgT\")]\n",
    "    \n",
    "    #Fun fact: this project first started with me scraping sleeping bags. The name 'bag' is leftover from that.\n",
    "    # Get the product IDs out of links\n",
    "    for bag in bags:\n",
    "        pids.append(re.findall(r\"(\\d{6})\", bag)[0]) \n",
    "    \n",
    "    # I am setting all outlet items to be priced at-99, since I assume their price has dropped significantly recently (ish)\n",
    "    # I'll set prices here\n",
    "    for bag in bags:\n",
    "        if bag[20:30] == 'rei-garage':\n",
    "            prices.append('-99')\n",
    "        else:\n",
    "            d=requests.get(bag).text\n",
    "            time.sleep(2)\n",
    "            html = BeautifulSoup(d, 'html.parser')\n",
    "            price = re.findall(r'(\\d{0,5}\\.\\d{1,2})', str(html.find_all('span', class_='price-value')))\n",
    "            if price == []:\n",
    "                prices.append(-99)\n",
    "            else:\n",
    "                prices.append(price[0])\n",
    "\n",
    "    # Create the data frame of prices for each product, and write it. Note that it is hilariously small.\n",
    "    # Again, you might prefer doing prices differently. \n",
    "    pricesDF = pd.DataFrame({'pid':pids,\n",
    "                            'price':prices})\n",
    "\n",
    "    pricesDF.to_csv(f'PRICES/REI_{product}_prices.csv')\n",
    "    \n",
    "    # Now the good stuff! Call the API to get reviews, questions, other stuff\n",
    "    for pid in pids:\n",
    "        params = {\n",
    "            # TO GET PARAMETER INFO:\n",
    "            # 1) GO TO A PRODUCT ON REI.COM\n",
    "            # 2) RIGHT CLICK\n",
    "            # 3) INSPECT ELEMENT (ON SAFARI)\n",
    "            # 4) GO TO NETWORK > BATCH.JSON > HEADERS\n",
    "        'passkey': '',#This changes semi-frequently\n",
    "        'apiversion': '5.5',\n",
    "        'displaycode': '15372-en_us',\n",
    "        'resource.q0': 'products',\n",
    "        'filter.q0': f'id:eq:{pid}',\n",
    "        'stats.q0': 'questions,reviews',\n",
    "        'filteredstats.q0': 'questions,reviews',\n",
    "        'filter_questions.q0': 'contentlocale:eq:en*,en_US',\n",
    "        'filter_answers.q0': 'contentlocale:eq:en*,en_US',\n",
    "        'filter_reviews.q0': 'contentlocale:eq:en*,en_US',\n",
    "        'filter_reviewcomments.q0': 'contentlocale:eq:en*,en_US',\n",
    "        'resource.q1': 'questions',\n",
    "        'filter.q1': f'productid:eq:{pid}',\n",
    "        'sort.q1': 'totalanswercount:desc',\n",
    "        'stats.q1': 'questions',\n",
    "        'filteredstats.q1': 'questions',\n",
    "        'include.q1': 'authors,products,answers',\n",
    "        'filter_questions.q1': 'contentlocale:eq:en*,en_US',\n",
    "        'filter_answers.q1': 'contentlocale:eq:en*,en_US',\n",
    "        'limit.q1': '20',\n",
    "        'offset.q1': '0',\n",
    "        'limit_answers.q1': '10',\n",
    "        'resource.q2': 'reviews',\n",
    "        'filter.q2': f'productid:eq:{pid}',\n",
    "        'sort.q2': 'submissiontime:desc',\n",
    "        'stats.q2': 'reviews',\n",
    "        'filteredstats.q2': 'reviews',\n",
    "        'include.q2': 'authors,products,comments',\n",
    "        'filter_reviews.q2': 'contentlocale:eq:en*,en_US',\n",
    "        'filter_reviewcomments.q2': 'contentlocale:eq:en*,en_US',\n",
    "        'filter_comments.q2': 'contentlocale:eq:en*,en_US',\n",
    "        'limit.q2': '100',\n",
    "        'offset.q2': '0',\n",
    "        'limit_comments.q2': '20',\n",
    "        'callback': 'JSONPHandler'\n",
    "    }\n",
    "        #Get that stuff!\n",
    "        req = requests.get(url, headers = headers, params = params)\n",
    "        data_json = req.text.split(\"(\", 1)[1].strip(\")\") # Convert to json\n",
    "        batched_results = json.loads(data_json)['BatchedResults']\n",
    "        \n",
    "        # Write the raw JSON so you can use it later if needed\n",
    "        if batched_results['q0']['Results']==[]:\n",
    "            continue\n",
    "        products_json = batched_results['q0']['Results'][0]\n",
    "        with open(f'JSONs/REI_{pid}_batched_results.json', 'w') as outfile:\n",
    "            json.dump(products_json, outfile)\n",
    "        \n",
    "        # Scrape questions to use if desired\n",
    "        questions = pd.DataFrame(batched_results['q1']['Results'])\n",
    "        questions.to_csv(f'QUESTIONS/REI_{pid}_questions.csv')  # Write them as csv\n",
    "        \n",
    "        # Scrape reviews\n",
    "        reviews = pd.DataFrame(batched_results['q2']['Results'])\n",
    "        reviews.to_csv(f'REVIEWS/REI_{pid}_reviews.csv')  \n",
    "        \n",
    "        # Product overall information\n",
    "        ovr = pd.DataFrame({'Name': 'NA' if products_json['Name'] == {} else[products_json['Name']],\n",
    "                            'Active': 'NA' if products_json['Active'] == {} else[products_json['Active']],\n",
    "                            'Brand': 'NA' if products_json['Brand'] == {} else [products_json['Brand']['Id']],\n",
    "                            'reviews': 'NA' if products_json['TotalReviewCount'] == {} else [products_json['TotalReviewCount']],\n",
    "                            'pid': f'{pid}', \n",
    "                            'ovr': 'NA' if products_json['ReviewStatistics']['AverageOverallRating'] == {} else[products_json['ReviewStatistics']['AverageOverallRating']],\n",
    "                           'desc': 'NA' if products_json['Description'] == {} else [products_json['Description']],\n",
    "                           'ProductPageUrl': 'NA' if products_json['ProductPageUrl'] == {} else [products_json['ProductPageUrl']],\n",
    "                            'category': f'{product}',\n",
    "                           })\n",
    "        ovr.to_csv(f'OVRs/REI_{pid}_ovr.csv')\n",
    "        time.sleep(15) #Rest, so that you don't overload REI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
