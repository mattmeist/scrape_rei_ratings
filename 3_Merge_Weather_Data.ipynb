{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### There are two ways to merge weather data with the REI reviews\n",
    "#### Option A: Download weather data, put it into folders by year, and run blocks 2&3.\n",
    "##### ^Only do this if you don't want to use the same time frame and data I do\n",
    "#### Option B (Better!): Just use the data I posted on OSF, and skip to block 4 (this is #2)\n",
    "\n",
    "# Create list of all file names to read in\n",
    "all_filenames = []\n",
    "years = ['2016',\n",
    "         '2017',\n",
    "         '2018',\n",
    "        '2019',\n",
    "        '2020',\n",
    "        '2021',\n",
    "        '2022',\n",
    "        '2023']\n",
    "\n",
    "for year in years:\n",
    "    for i in glob.glob(f'Weather/{year}/*.csv'):\n",
    "        all_filenames.append(i)\n",
    "\n",
    "        # Which attributes do you want?\n",
    "atts = ['STATION',\n",
    "        'DATE',\n",
    "        'LATITUDE',\n",
    "        'LONGITUDE',\n",
    "        'ELEVATION',\n",
    "        'TEMP',\n",
    "        'MAX',\n",
    "        'MIN',\n",
    "        'PRCP',\n",
    "        'PRCP_ATTRIBUTES']\n",
    "\n",
    "## Now, combine all files in the list\n",
    "# Run the line below.  OR just read in the below data, and skip the next cell\n",
    "d = pd.concat([pd.read_csv(f, usecols = atts) for f in all_filenames])\n",
    "\n",
    "d = d[d['STATION'] != 99401499999]\n",
    "d = d[d['STATION'] != 99999900178]\n",
    "d = d[d['STATION'] != 72055399999]\n",
    "d = d[d['STATION'] != 74594493784]\n",
    "d = d[d['STATION'] != 99404899999]\n",
    "\n",
    "# Replace messed up values\n",
    "d[['TEMP']] = d[['TEMP']].replace({9999.9: np.nan})\n",
    "d[['MAX']] = d[['MAX']].replace({9999.9: np.nan})\n",
    "d[['MIN']] = d[['MIN']].replace({9999.9: np.nan})\n",
    "d[['PRCP']] = d[['PRCP']].replace({99.99: np.nan})\n",
    "\n",
    "# Convert the 'date' column to datetime format\n",
    "d['DATE_unix'] = pd.to_datetime(d['DATE'])\n",
    "\n",
    "# Extract year, month, and day from the date column\n",
    "d['year'] = d['DATE_unix'].dt.year\n",
    "d['month'] = d['DATE_unix'].dt.month\n",
    "d['day'] = d['DATE_unix'].dt.day\n",
    "\n",
    "# Create a new column for grouping by year\n",
    "d['group_year'] = d['DATE_unix'].dt.year\n",
    "\n",
    "# Group by station, date, and year, then calculate the average for each group\n",
    "d = d.groupby(['STATION', 'DATE', 'DATE_unix', 'LATITUDE', 'LONGITUDE', 'group_year', 'month', 'day']).agg({\n",
    "    'MAX': 'mean',\n",
    "    'MIN': 'mean',\n",
    "    'TEMP': 'mean',\n",
    "    'PRCP': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate the normal values based on averages from all years\n",
    "d['normMAX'] = d.groupby(['STATION', 'month', 'day'])['MAX'].transform('mean')\n",
    "d['normMIN'] = d.groupby(['STATION', 'month', 'day'])['MIN'].transform('mean')\n",
    "d['normTEMP'] = d.groupby(['STATION', 'month', 'day'])['TEMP'].transform('mean')\n",
    "d['normPRCP'] = d.groupby(['STATION', 'month', 'day'])['PRCP'].transform('mean')\n",
    "\n",
    "\n",
    "### Now! Calculate averages for 3 and 7 days weather\n",
    "\n",
    "# Write a date that is too early to calculate 7-day average\n",
    "tooearly7 = 1452247200\n",
    "# Write a date that is too early to calculate 3-day average\n",
    "tooearly3 = 1451815200\n",
    "\n",
    "# Make date unix so we can use the above\n",
    "d['DATE_unix'] = pd.to_datetime(d['DATE']).map(pd.Timestamp.timestamp)\n",
    "d.reset_index(drop = True, inplace=True) #write window fn\n",
    "window_size7 = 7\n",
    "window_size3 = 3\n",
    "\n",
    "i = 0\n",
    "# Initialize an empty list to store moving averages\n",
    "MAX_averages7 = ['NA','NA','NA','NA','NA','NA']\n",
    "MIN_averages7 = ['NA','NA','NA','NA','NA','NA']\n",
    "TEMP_averages7 = ['NA','NA','NA','NA','NA','NA']\n",
    "PRCP_averages7 = ['NA','NA','NA','NA','NA','NA']\n",
    "\n",
    "MAX_averages3 = ['NA','NA']\n",
    "MIN_averages3 = ['NA','NA']\n",
    "TEMP_averages3 = ['NA','NA']\n",
    "PRCP_averages3 = ['NA','NA']\n",
    "  \n",
    "# Loop through the array to consider\n",
    "# every window of size 7\n",
    "while i < len(d.DATE) - window_size7 + 1:\n",
    "    if d.DATE_unix[i] < tooearly7:\n",
    "        MAX_averages7.append('NA')\n",
    "        MIN_averages7.append('NA')\n",
    "        TEMP_averages7.append('NA')\n",
    "        PRCP_averages7.append('NA')\n",
    "        i += 1\n",
    "    else:\n",
    "        MAX_averages7.append(round(np.nansum(d.MAX[i : i + window_size7]) / window_size7, 4))\n",
    "        MIN_averages7.append(round(np.nansum(d.MIN[i : i + window_size7]) / window_size7, 4))\n",
    "        TEMP_averages7.append(round(np.nansum(d.TEMP[i : i + window_size7]) / window_size7, 4))\n",
    "        PRCP_averages7.append(round(np.nansum(d.PRCP[i : i + window_size7]) / window_size7, 4))\n",
    "        i += 1\n",
    "\n",
    "d['avg7MAX'] = MAX_averages7\n",
    "d['avg7MIN'] = MIN_averages7\n",
    "d['avg7TEMP'] = TEMP_averages7\n",
    "d['avg7PRCP'] = PRCP_averages7\n",
    "\n",
    "# Loop through the array to consider\n",
    "# every window of size 3\n",
    "i=0\n",
    "while i < len(d.DATE) - window_size3 + 1:\n",
    "    if d.DATE_unix[i] < tooearly3:\n",
    "        MAX_averages3.append('NA')\n",
    "        MIN_averages3.append('NA')\n",
    "        TEMP_averages3.append('NA')\n",
    "        PRCP_averages3.append('NA')\n",
    "        i += 1\n",
    "    else:\n",
    "        MAX_averages3.append(round(np.nansum(d.MAX[i : i + window_size3]) / window_size3, 2))\n",
    "        MIN_averages3.append(round(np.nansum(d.MIN[i : i + window_size3]) / window_size3, 2))\n",
    "        TEMP_averages3.append(round(np.nansum(d.TEMP[i : i + window_size3]) / window_size3, 2))\n",
    "        PRCP_averages3.append(round(np.nansum(d.PRCP[i : i + window_size3]) / window_size3, 2))\n",
    "        i += 1\n",
    "\n",
    "d['avg3MAX'] = MAX_averages3\n",
    "d['avg3MIN'] = MIN_averages3\n",
    "d['avg3TEMP'] = TEMP_averages3\n",
    "d['avg3PRCP'] = PRCP_averages3\n",
    "d = d.drop(columns=['DATE_unix'])\n",
    "\n",
    "# If you wrote a new NOAA file, run this line to save it\n",
    "d.to_csv( \"NOAA_160101_231127.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'unicodeescape' codec can't decode bytes in position 101587-101588: truncated \\uXXXX escape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTATION\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: find_station(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat_mid\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlng_mid\u001b[39m\u001b[38;5;124m'\u001b[39m]), \n\u001b[1;32m     25\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Now, read in the reviews. Read in ALL data this time (df was only reading in lat/long)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m revs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREI_reviews_located.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124municode_escape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# For speed, only include reviews with locations\u001b[39;00m\n\u001b[1;32m     31\u001b[0m revs \u001b[38;5;241m=\u001b[39m revs[revs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat_mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotnull()]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1779\u001b[0m         nrows\n\u001b[1;32m   1780\u001b[0m     )\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:1965\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'unicodeescape' codec can't decode bytes in position 101587-101588: truncated \\uXXXX escape"
     ]
    }
   ],
   "source": [
    "## If you downloaded the weather data from OSF, skip to here and uncomment the line below\n",
    "#d = pd.read_csv(\"NOAA_160101_231127.csv\") \n",
    "\n",
    "# Station data\n",
    "stations = d[['STATION','LATITUDE','LONGITUDE']].drop_duplicates()\n",
    "# Reviews data (for finding stations closest to a review)\n",
    "df = pd.read_csv(\"REI_reviews_located.csv\", usecols=['lat_mid', 'lng_mid'], encoding='utf-8')\n",
    "df = df[df['lat_mid'].notnull()].drop_duplicates()\n",
    "\n",
    "# Now, write the function that finds the closest station to each review\n",
    "stations = stations.set_index('STATION')\n",
    "def dist(lat1, long1, lat2, long2):\n",
    "    return ((lat1*110-lat2*110)**2+(long1*84-long2*84)**2)**(1/2)\n",
    "\n",
    "def find_station(lat, long):\n",
    "    stations['distance'] = stations.apply(\n",
    "        lambda row: dist(lat, long, row['LATITUDE'], row['LONGITUDE']), \n",
    "        axis=1)\n",
    "    return stations['distance'].idxmin()\n",
    "\n",
    "# Find those stations for each unique lat/long from the reviews\n",
    "df['STATION'] = df.apply(\n",
    "    lambda row: find_station(row['lat_mid'], row['lng_mid']), \n",
    "    axis=1)\n",
    "\n",
    "# Now, read in the reviews. Read in ALL data this time (df was only reading in lat/long)\n",
    "revs = pd.read_csv(\"REI_reviews_located.csv\", encoding = 'utf-8')\n",
    "\n",
    "# For speed, only include reviews with locations\n",
    "revs = revs[revs['lat_mid'].notnull()]\n",
    "\n",
    "# Merge reviews with weather stations closest to each\n",
    "revs = revs.merge(df, how = 'left', on=['lat_mid', 'lng_mid'])\n",
    "\n",
    "# Format date in the weather data so we can merge those two\n",
    "d['date'] = d['DATE'].str.slice(stop=10)\n",
    "# Format date in the reviews data so we can merge those two\n",
    "revs['date'] = revs['SubmissionTime'].str.slice(stop=10)\n",
    "\n",
    "# Merge and write the reviews data\n",
    "revs = revs.merge(d, how = 'left', on=['STATION', 'date'])\n",
    "\n",
    "revs.to_csv('REI_Analysis.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wb/hdmmy2hd4j95wt0688czs2w5jf1pkl/T/ipykernel_47193/3975984664.py:2: DtypeWarning: Columns (51,52,55,57,62,63,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  revs = pd.read_csv(\"REI_reviews_located.csv\", encoding = 'utf-8')\n"
     ]
    }
   ],
   "source": [
    "# Now, read in the reviews. Read in ALL data this time (df was only reading in lat/long)\n",
    "revs = pd.read_csv(\"REI_reviews_located.csv\", encoding = 'utf-8')\n",
    "\n",
    "# For speed, only include reviews with locations\n",
    "revs = revs[revs['lat_mid'].notnull()]\n",
    "\n",
    "# Merge reviews with weather stations closest to each\n",
    "revs = revs.merge(df, how = 'left', on=['lat_mid', 'lng_mid'])\n",
    "\n",
    "# Format date in the weather data so we can merge those two\n",
    "d['date'] = d['DATE'].str.slice(stop=10)\n",
    "# Format date in the reviews data so we can merge those two\n",
    "revs['date'] = revs['SubmissionTime'].str.slice(stop=10)\n",
    "\n",
    "# Merge and write the reviews data\n",
    "revs = revs.merge(d, how = 'left', on=['STATION', 'date'])\n",
    "\n",
    "revs.to_csv('REI_Analysis.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
