{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### There are two ways to merge weather data with the REI reviews\n",
    "#### Option A: Download weather data, put it into folders by year, and run blocks 2&3.\n",
    "##### ^Only do this if you don't want to use the same time frame and data I do\n",
    "#### Option B (Better!): Just use the data I posted on OSF, and skip to block 4 (this is #2)\n",
    "\n",
    "# Create list of all file names to read in\n",
    "all_filenames = []\n",
    "years = ['2016',\n",
    "         '2017',\n",
    "         '2018',\n",
    "        '2019',\n",
    "        '2020',\n",
    "        '2021',\n",
    "        '2022']\n",
    "\n",
    "for year in years:\n",
    "    for i in glob.glob(f'{year}/*.csv'):\n",
    "        all_filenames.append(i)\n",
    "\n",
    "        # Which attributes do you want?\n",
    "atts = ['STATION',\n",
    "        'DATE',\n",
    "        'LATITUDE',\n",
    "        'LONGITUDE',\n",
    "        'ELEVATION',\n",
    "        'TEMP',\n",
    "        'MAX',\n",
    "        'MIN',\n",
    "        'PRCP',\n",
    "        'PRCP_ATTRIBUTES']\n",
    "\n",
    "## Now, combine all files in the list\n",
    "# Run the line below.  OR just read in the below data, and skip the next cell\n",
    "d = pd.concat([pd.read_csv(f, usecols = atts) for f in all_filenames])\n",
    "\n",
    "d = d[d['STATION'] != 99401499999]\n",
    "d = d[d['STATION'] != 99999900178]\n",
    "d = d[d['STATION'] != 72055399999]\n",
    "d = d[d['STATION'] != 74594493784]\n",
    "d = d[d['STATION'] != 99404899999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now! Calculate averages for 3 and 7 days weather\n",
    "\n",
    "# Write a date that is too early to calculate 7-day average\n",
    "tooearly7 = 1452247200\n",
    "# Write a date that is too early to calculate 3-day average\n",
    "tooearly3 = 1451815200\n",
    "\n",
    "# Make date unix so we can use the above\n",
    "d['DATE_unix'] = pd.to_datetime(d.DATE).map(pd.Timestamp.timestamp)\n",
    "d.reset_index(drop = True, inplace=True) #write window fn\n",
    "window_size7 = 7\n",
    "window_size3 = 3\n",
    "\n",
    "i = 0\n",
    "# Initialize an empty list to store moving averages\n",
    "MAX_averages7 = ['NA','NA','NA','NA','NA','NA']\n",
    "MIN_averages7 = ['NA','NA','NA','NA','NA','NA']\n",
    "TEMP_averages7 = ['NA','NA','NA','NA','NA','NA']\n",
    "PRCP_averages7 = ['NA','NA','NA','NA','NA','NA']\n",
    "\n",
    "MAX_averages3 = ['NA','NA']\n",
    "MIN_averages3 = ['NA','NA']\n",
    "TEMP_averages3 = ['NA','NA']\n",
    "PRCP_averages3 = ['NA','NA']\n",
    "  \n",
    "# Loop through the array to consider\n",
    "# every window of size 7\n",
    "while i < len(d.DATE) - window_size7 + 1:\n",
    "    if d.DATE_unix[i] < tooearly7:\n",
    "        MAX_averages7.append('NA')\n",
    "        MIN_averages7.append('NA')\n",
    "        TEMP_averages7.append('NA')\n",
    "        PRCP_averages7.append('NA')\n",
    "        i += 1\n",
    "    else:\n",
    "        MAX_averages7.append(round(sum(d.MAX[i : i + window_size7]) / window_size7, 4))\n",
    "        MIN_averages7.append(round(sum(d.MIN[i : i + window_size7]) / window_size7, 4))\n",
    "        TEMP_averages7.append(round(sum(d.TEMP[i : i + window_size7]) / window_size7, 4))\n",
    "        PRCP_averages7.append(round(sum(d.PRCP[i : i + window_size7]) / window_size7, 4))\n",
    "        i += 1\n",
    "\n",
    "d['avg7MAX'] = MAX_averages7\n",
    "d['avg7MIN'] = MIN_averages7\n",
    "d['avg7TEMP'] = TEMP_averages7\n",
    "d['avg7PRCP'] = PRCP_averages7\n",
    "\n",
    "# Loop through the array to consider\n",
    "# every window of size 3\n",
    "i=0\n",
    "while i < len(d.DATE) - window_size3 + 1:\n",
    "    if d.DATE_unix[i] < tooearly3:\n",
    "        MAX_averages3.append('NA')\n",
    "        MIN_averages3.append('NA')\n",
    "        TEMP_averages3.append('NA')\n",
    "        PRCP_averages3.append('NA')\n",
    "        i += 1\n",
    "    else:\n",
    "        MAX_averages3.append(round(sum(d.MAX[i : i + window_size3]) / window_size3, 2))\n",
    "        MIN_averages3.append(round(sum(d.MIN[i : i + window_size3]) / window_size3, 2))\n",
    "        TEMP_averages3.append(round(sum(d.TEMP[i : i + window_size3]) / window_size3, 2))\n",
    "        PRCP_averages3.append(round(sum(d.PRCP[i : i + window_size3]) / window_size3, 2))\n",
    "        i += 1\n",
    "\n",
    "d['avg3MAX'] = MAX_averages3\n",
    "d['avg3MIN'] = MIN_averages3\n",
    "d['avg3TEMP'] = TEMP_averages3\n",
    "d['avg3PRCP'] = PRCP_averages3\n",
    "d = d.drop(columns=['DATE_unix'])\n",
    "\n",
    "# If you wrote a new NOAA file, run this line to save it\n",
    "d.to_csv( \"NOAA_160101_221001.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you downloaded the weather data from OSF, skip to here and uncomment these lines\n",
    "# Weather data\n",
    "d = pd.read_csv(\"NOAA_160101_221001.csv\") \n",
    "# Station data\n",
    "stations = d[['STATION','LATITUDE','LONGITUDE']].drop_duplicates()\n",
    "# Reviews data (for finding stations closest to a review)\n",
    "df = pd.read_csv(\"REI_reviews_located_clean.csv\", usecols = ['lat_mid', 'lng_mid'], encoding = 'unicode_escape')\n",
    "df = df[df['lat_mid'].notnull()].drop_duplicates()\n",
    "\n",
    "# Now, write the function that finds the closest station to each review\n",
    "stations = stations.set_index('STATION')\n",
    "def dist(lat1, long1, lat2, long2):\n",
    "    return ((lat1*110-lat2*110)**2+(long1*84-long2*84)**2)**(1/2)\n",
    "\n",
    "def find_station(lat, long):\n",
    "    stations['distance'] = stations.apply(\n",
    "        lambda row: dist(lat, long, row['LATITUDE'], row['LONGITUDE']), \n",
    "        axis=1)\n",
    "    return stations['distance'].idxmin()\n",
    "\n",
    "# Find those stations for each unique lat/long from the reviews\n",
    "df['STATION'] = df.apply(\n",
    "    lambda row: find_station(row['lat_mid'], row['lng_mid']), \n",
    "    axis=1)\n",
    "\n",
    "# Now, read in the reviews. Read in ALL data this time (df was only reading in lat/long)\n",
    "revs = pd.read_csv(\"REI_reviews_located_clean.csv\", encoding = 'unicode_escape')\n",
    "\n",
    "# For speed, only include reviews with locations\n",
    "revs = revs[revs['lat_mid'].notnull()]\n",
    "\n",
    "# Merge reviews with weather stations closest to each\n",
    "revs = revs.merge(df, how = 'left', on=['lat_mid', 'lng_mid'])\n",
    "\n",
    "# Format date in the weather data so we can merge those two\n",
    "d['date'] = d['DATE'].str.slice(stop=10)\n",
    "# Format date in the reviews data so we can merge those two\n",
    "revs['date'] = revs['SubmissionTime'].str.slice(stop=10)\n",
    "\n",
    "# Merge and write the reviews data\n",
    "revs = revs.merge(d, how = 'left', on=['STATION', 'date'])\n",
    "\n",
    "revs.to_csv('REI_located_weathered_reviews.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
